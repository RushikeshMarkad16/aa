{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34b9d77",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "  1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, \n",
    "     stop words removal, Stemming and Lemmatization.\n",
    "  2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "     Frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b77ac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcaa7540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "corpus = \"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bb838",
   "metadata": {},
   "source": [
    "# Task1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384bb6d",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "719edc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8975bea",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cbbc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(corpus)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5994a8c",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c933351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(corpus)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558f1bf",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc234904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7289782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"needn't\", 'o', 'them', 'before', 'too', \"couldn't\", 'both', 'those', 'didn', 'once', 'few', 'again', 'over', 'whom', 'he', 'further', 'myself', 'himself', 'out', 'if', 'between', 'no', 'some', \"hasn't\", 'itself', \"weren't\", 'themselves', 'shouldn', 'll', 'd', \"should've\", 'it', 'this', 'then', 'doing', 'just', 's', 'under', 'the', \"shouldn't\", 'hers', 'have', 'm', 'here', \"don't\", 'being', \"hadn't\", 'against', 'needn', 'does', 'by', 'couldn', \"wasn't\", 'isn', 'are', 'below', 'about', 'don', \"won't\", 'only', 'her', 'my', 'do', 'will', 'in', 'off', 'very', 'i', 'has', 'aren', 'haven', 'any', 'ourselves', 'at', 'theirs', \"you're\", 'nor', 'is', 'each', 'him', 'been', 'most', 're', 'how', 't', 'were', 'so', 'was', 'now', 'these', 'during', 'on', 'am', 'doesn', 'ma', 'me', \"didn't\", 'a', 'not', 'mustn', \"shan't\", 'that', 'you', 'weren', 'hasn', 'yours', 'of', 'did', 'than', \"it's\", 'there', 'to', 'wouldn', 'its', 'when', 'yourselves', 'or', 'as', 'for', 'but', 'above', 'from', \"mustn't\", 'can', 'hadn', \"she's\", \"you'll\", \"aren't\", 'into', 'up', \"isn't\", 'because', \"doesn't\", 'why', 'down', 'she', 'which', 'your', 'own', 'their', 'yourself', 'who', 'herself', \"you'd\", 'having', 'an', 'what', 'more', 'won', \"wouldn't\", \"you've\", 'all', 'ours', 'wasn', 'his', \"haven't\", 'we', 'and', 'be', 'should', 'our', 'through', \"mightn't\", 'shan', 'while', 'such', 'where', 'they', 've', 'ain', 'until', 'had', 'same', 'y', 'mightn', 'with', 'other', 'after', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85df8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = []\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized words:\",tokenized_word)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7bb47",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebb98dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb686221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc4f7b",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "faeb8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "059f6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eeaa4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449c989",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5897bc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " (',', ','),\n",
       " ('how', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('today', 'NN'),\n",
       " ('?', '.'),\n",
       " ('The', 'DT'),\n",
       " ('weather', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('great', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('city', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('awesome', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('sky', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('pinkish-blue', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('You', 'PRP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('eat', 'VB'),\n",
       " ('cardboard', 'NN')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb397ab",
   "metadata": {},
   "source": [
    "# Task2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afbcfd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data', 'century', '21st', 'science', 'key', 'of', 'learning', 'hardest', 'job', 'machine', 'the', 'Science', 'data', 'is', 'for'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Data Science is the hardest job of the 21st century\"\n",
    "second_sentence = \"machine learning is the key for data science\"\n",
    "\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")\n",
    "\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b75f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data': 1, 'century': 1, '21st': 1, 'science': 0, 'key': 0, 'of': 1, 'learning': 0, 'hardest': 1, 'job': 1, 'machine': 0, 'the': 2, 'Science': 1, 'data': 0, 'is': 1, 'for': 0}\n",
      "{'Data': 0, 'century': 0, '21st': 0, 'science': 1, 'key': 1, 'of': 0, 'learning': 1, 'hardest': 0, 'job': 0, 'machine': 1, 'the': 1, 'Science': 0, 'data': 1, 'is': 1, 'for': 1}\n"
     ]
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "print(wordDictA)\n",
    "print(wordDictB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ee0fb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>century</th>\n",
       "      <th>21st</th>\n",
       "      <th>science</th>\n",
       "      <th>key</th>\n",
       "      <th>of</th>\n",
       "      <th>learning</th>\n",
       "      <th>hardest</th>\n",
       "      <th>job</th>\n",
       "      <th>machine</th>\n",
       "      <th>the</th>\n",
       "      <th>Science</th>\n",
       "      <th>data</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data  century  21st  science  key  of  learning  hardest  job  machine  \\\n",
       "0     1        1     1        0    0   1         0        1    1        0   \n",
       "1     0        0     0        1    1   0         1        0    0        1   \n",
       "\n",
       "   the  Science  data  is  for  \n",
       "0    2        1     0   1    0  \n",
       "1    1        0     1   1    1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf6275",
   "metadata": {},
   "source": [
    "# TF (Term Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17a3463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6313b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data  century  21st  science    key   of  learning  hardest  job  machine  \\\n",
      "0   0.1      0.1   0.1    0.000  0.000  0.1     0.000      0.1  0.1    0.000   \n",
      "1   0.0      0.0   0.0    0.125  0.125  0.0     0.125      0.0  0.0    0.125   \n",
      "\n",
      "     the  Science   data     is    for  \n",
      "0  0.200      0.1  0.000  0.100  0.000  \n",
      "1  0.125      0.0  0.125  0.125  0.125  \n"
     ]
    }
   ],
   "source": [
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e60f8d",
   "metadata": {},
   "source": [
    "# IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e243323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        cnt = 0\n",
    "        for doc in docList:\n",
    "            if(doc[word] != 0):\n",
    "                cnt += 1\n",
    "        idfDict[word] = cnt\n",
    "#     print(idfDict)\n",
    "        \n",
    "        \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) ))\n",
    "        \n",
    "    return(idfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e6d6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data': 1, 'century': 1, '21st': 1, 'science': 1, 'key': 1, 'of': 1, 'learning': 1, 'hardest': 1, 'job': 1, 'machine': 1, 'the': 2, 'Science': 1, 'data': 1, 'is': 2, 'for': 1}\n",
      "{'Data': 0.3010299956639812, 'century': 0.3010299956639812, '21st': 0.3010299956639812, 'science': 0.3010299956639812, 'key': 0.3010299956639812, 'of': 0.3010299956639812, 'learning': 0.3010299956639812, 'hardest': 0.3010299956639812, 'job': 0.3010299956639812, 'machine': 0.3010299956639812, 'the': 0.0, 'Science': 0.3010299956639812, 'data': 0.3010299956639812, 'is': 0.0, 'for': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "print(idfs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
