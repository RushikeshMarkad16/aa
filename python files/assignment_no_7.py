# -*- coding: utf-8 -*-
"""Assignment_No_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nPsPqENscVfxmwsQNMnnrRlDj10RyT6a

# Text Analytics
  1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, 
     stop words removal, Stemming and Lemmatization.
  2. Create representation of document by calculating Term Frequency and Inverse Document
     Frequency.
"""

import nltk
import pandas as pd
import math

from nltk.corpus import inaugural
corpus = "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard"
print(corpus)

"""# Task1:

## Tokenization
"""

from nltk.tokenize import word_tokenize,sent_tokenize

"""## Sentence Tokenization"""

tokenized_text = sent_tokenize(corpus)
print(tokenized_text)

"""## Word Tokenization"""

tokenized_word = word_tokenize(corpus)
print(tokenized_word)

"""## Stopwords Removal"""

from nltk.corpus import stopwords

stop_words=set(stopwords.words("english"))
print(stop_words)

filtered_sent = []
for w in tokenized_word:
    if w not in stop_words:
        filtered_sent.append(w)
print("Tokenized words:",tokenized_word)
print("Filterd Sentence:",filtered_sent)

"""## Stemming"""

from nltk.stem import PorterStemmer

ps = PorterStemmer()
stemmed_words = []

for w in filtered_sent:
    stemmed_words.append(ps.stem(w))

print("Filtered Sentence:",filtered_sent)
print("Stemmed Sentence:",stemmed_words)

"""## Lemmatization"""

from nltk.stem.wordnet import WordNetLemmatizer

lem = WordNetLemmatizer()
lemmatized_words = []

for w in filtered_sent:
    lemmatized_words.append(lem.lemmatize(w))

print("Filtered Sentence:",filtered_sent)
print("Stemmed Sentence:",lemmatized_words)

word = "flying"
print("Lemmatized Word:",lem.lemmatize(word,"v"))
print("Stemmed Word:",ps.stem(word))

"""## POS Tagging"""

nltk.pos_tag(tokenized_word)

"""# Task2:"""

first_sentence = "Data Science is the hardest job of the 21st century"
second_sentence = "machine learning is the key for data science"

first_sentence = first_sentence.split(" ")
second_sentence = second_sentence.split(" ")

total= set(first_sentence).union(set(second_sentence))
print(total)

wordDictA = dict.fromkeys(total, 0) 
wordDictB = dict.fromkeys(total, 0)
for word in first_sentence:
    wordDictA[word]+=1
    
for word in second_sentence:
    wordDictB[word]+=1
    
print(wordDictA)
print(wordDictB)

pd.DataFrame([wordDictA, wordDictB])

"""# TF (Term Frequency)"""

def computeTF(wordDict, doc):
    tfDict = {}
    corpusCount = len(doc)
    for word, count in wordDict.items():
        tfDict[word] = count/float(corpusCount)
    return(tfDict)

#running our sentences through the tf function:
tfFirst = computeTF(wordDictA, first_sentence)
tfSecond = computeTF(wordDictB, second_sentence)

#Converting to dataframe for visualization
tf = pd.DataFrame([tfFirst, tfSecond])
print(tf)

"""# IDF (Inverse Document Frequency)"""

def computeIDF(docList):
    idfDict = {}
    N = len(docList)
    
    idfDict = dict.fromkeys(docList[0].keys(), 0)
    
    for word, val in idfDict.items():
        cnt = 0
        for doc in docList:
            if(doc[word] != 0):
                cnt += 1
        idfDict[word] = cnt
#     print(idfDict)
        
        
    for word, val in idfDict.items():
        idfDict[word] = math.log10(N / (float(val) ))
        
    return(idfDict)

idfs = computeIDF([wordDictA, wordDictB])
print(idfs)